{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_FGVISEp0Z4",
        "outputId": "4cbee4b5-32ae-442d-e88a-dae73b0a4bf7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.12/dist-packages (25.3)\n",
            "Requirement already satisfied: onnx in /usr/local/lib/python3.12/dist-packages (1.19.1)\n",
            "Requirement already satisfied: onnxruntime in /usr/local/lib/python3.12/dist-packages (1.23.2)\n",
            "Requirement already satisfied: onnxruntime-tools in /usr/local/lib/python3.12/dist-packages (1.7.0)\n",
            "Requirement already satisfied: onnxruntime-extensions in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.24.0+cu126)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (11.3.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.12/dist-packages (from onnx) (2.0.2)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (5.29.5)\n",
            "Requirement already satisfied: typing_extensions>=4.7.1 in /usr/local/lib/python3.12/dist-packages (from onnx) (4.15.0)\n",
            "Requirement already satisfied: ml_dtypes>=0.5.0 in /usr/local/lib/python3.12/dist-packages (from onnx) (0.5.4)\n",
            "Requirement already satisfied: coloredlogs in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (15.0.1)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.9.23)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (25.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.12/dist-packages (from onnxruntime) (1.14.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from onnxruntime-tools) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.12/dist-packages (from onnxruntime-tools) (9.0.0)\n",
            "Requirement already satisfied: py3nvml in /usr/local/lib/python3.12/dist-packages (from onnxruntime-tools) (0.2.7)\n",
            "Requirement already satisfied: torch==2.9.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.9.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (75.2.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch==2.9.0->torchvision) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy->onnxruntime) (1.3.0)\n",
            "Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.12/dist-packages (from coloredlogs->onnxruntime) (10.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch==2.9.0->torchvision) (3.0.3)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.12/dist-packages (from py3nvml->onnxruntime-tools) (1.0.2)\n",
            "Collecting onnxscript\n",
            "  Downloading onnxscript-0.5.6-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: ml_dtypes in /usr/local/lib/python3.12/dist-packages (from onnxscript) (0.5.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from onnxscript) (2.0.2)\n",
            "Collecting onnx_ir<2,>=0.1.12 (from onnxscript)\n",
            "  Downloading onnx_ir-0.1.12-py3-none-any.whl.metadata (3.2 kB)\n",
            "Requirement already satisfied: onnx>=1.16 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (1.19.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from onnxscript) (25.0)\n",
            "Requirement already satisfied: typing_extensions>=4.10 in /usr/local/lib/python3.12/dist-packages (from onnxscript) (4.15.0)\n",
            "Requirement already satisfied: protobuf>=4.25.1 in /usr/local/lib/python3.12/dist-packages (from onnx>=1.16->onnxscript) (5.29.5)\n",
            "Downloading onnxscript-0.5.6-py3-none-any.whl (683 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m683.0/683.0 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m  \u001b[33m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnx_ir-0.1.12-py3-none-any.whl (129 kB)\n",
            "Installing collected packages: onnx_ir, onnxscript\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [onnxscript]\n",
            "\u001b[1A\u001b[2KSuccessfully installed onnx_ir-0.1.12 onnxscript-0.5.6\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies (run once in Colab / environment)\n",
        "!pip install --upgrade pip\n",
        "!pip install onnx onnxruntime onnxruntime-tools onnxruntime-extensions torchvision pillow tqdm matplotlib\n",
        "!pip install onnxscript\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2KpsEBJYqA8R",
        "outputId": "6221f407-389e-4314-9559-8a012c8437be"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using Colab cache for faster access to the 'imagenetmini-1000' dataset.\n",
            "Path to dataset files: /kaggle/input/imagenetmini-1000\n"
          ]
        }
      ],
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"ifigotin/imagenetmini-1000\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCzzWel9s5LQ"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 828
        },
        "id": "xXjGFc88p31N",
        "outputId": "fab73be9-38dc-4506-8791-184622474c3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PyTorch model (ResNet18 pretrained)...\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 156MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling forward order / latency / activation memory...\n",
            "Leaf modules executed: 52\n",
            "Computing sensitivity & scores (this may take time)...\n",
            "Top 10 scored modules:\n",
            "bn1                                      | score=1.337e+09 | lat=0.016521s | mem=25088.5KB | sens=1.152e-06\n",
            "layer1.1.conv2                           | score=1.096e+02 | lat=0.031730s | mem=6416.0KB | sens=3.617e-06\n",
            "layer1.0.bn2                             | score=1.084e+02 | lat=0.001205s | mem=6272.5KB | sens=1.844e-06\n",
            "layer4.1.conv2                           | score=1.016e+02 | lat=0.037423s | mem=10000.0KB | sens=4.609e-06\n",
            "layer4.0.conv2                           | score=9.997e+01 | lat=0.035935s | mem=10000.0KB | sens=4.572e-06\n",
            "layer3.1.conv2                           | score=9.595e+01 | lat=0.031435s | mem=3872.0KB | sens=3.617e-06\n",
            "layer2.1.conv2                           | score=9.500e+01 | lat=0.030656s | mem=3712.0KB | sens=3.569e-06\n",
            "layer1.1.bn2                             | score=7.509e+01 | lat=0.001175s | mem=6272.5KB | sens=2.148e-06\n",
            "layer1.1.bn1                             | score=6.492e+01 | lat=0.001260s | mem=6272.5KB | sens=2.312e-06\n",
            "layer1.0.conv2                           | score=5.729e+01 | lat=0.033897s | mem=6416.0KB | sens=6.110e-06\n",
            "Exporting FP32 model to ONNX with dynamic batch...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'onnxscript'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2857916367.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    415\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 417\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-2857916367.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Exporting FP32 model to ONNX with dynamic batch...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 365\u001b[0;31m     \u001b[0mexport_pytorch_to_onnx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpt_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFP32_ONNX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    366\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    367\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Collecting calibration images...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2857916367.py\u001b[0m in \u001b[0;36mexport_pytorch_to_onnx\u001b[0;34m(model, out_path, opset)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0mdummy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mIMAGE_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m     torch.onnx.export(\n\u001b[0m\u001b[1;32m     86\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdummy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0minput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"output\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/__init__.py\u001b[0m in \u001b[0;36mexport\u001b[0;34m(model, args, f, kwargs, verbose, input_names, output_names, opset_version, dynamo, external_data, dynamic_shapes, custom_translation_table, report, optimize, verify, profile, dump_exported_program, artifacts_dir, fallback, export_params, keep_initializers_as_inputs, dynamic_axes, training, operator_export_type, do_constant_folding, custom_opsets, export_modules_as_functions, autograd_inlining)\u001b[0m\n\u001b[1;32m    280\u001b[0m     \"\"\"\n\u001b[1;32m    281\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mdynamo\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mTrue\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExportedProgram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexporter\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_compat\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_compat.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_constants\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0monnx_constants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_internal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_import\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnxscript_apis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monnxscript_ir\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m from torch.onnx._internal.exporter import (\n\u001b[0m\u001b[1;32m     17\u001b[0m     \u001b[0m_constants\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0m_core\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/onnx/_internal/exporter/_core.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCallable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mLiteral\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0monnxscript\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0monnxscript\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0monnxscript\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'onnxscript'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "source": [
        "# selective_qdq_pipeline.py\n",
        "import os\n",
        "import time\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms, datasets, models\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from onnxruntime.quantization import (\n",
        "    quantize_static,\n",
        "    CalibrationDataReader,\n",
        "    QuantFormat,\n",
        "    QuantType\n",
        ")\n",
        "from PIL import Image\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# ----------------------------\n",
        "# CONFIG\n",
        "# ----------------------------\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_EVAL = 16         # batch used during ONNX eval (keep small if CPU)\n",
        "CALIB_BATCH = 8         # batch used by calibration reader\n",
        "CALIB_SAMPLES = 256     # number of images for calibration\n",
        "EVAL_SAMPLES = 512      # number of val images to evaluate\n",
        "OPSET = 13\n",
        "\n",
        "# Set this to your imagenet-mini root with train/val folders\n",
        "#DATA_ROOT = \"/root/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini\"\n",
        "DATA_ROOT = \"/kaggle/input/imagenetmini-1000/\"\n",
        "FP32_ONNX = \"resnet18_fp32.onnx\"\n",
        "QONNX_SEL_TEMPLATE = \"resnet18_qdq_selective_k{}.onnx\"\n",
        "QONNX_FULL = \"resnet18_qlinear_full.onnx\"  # optional full qlinear path\n",
        "\n",
        "# ----------------------------\n",
        "# DATA PREPROCESSING & LOADERS\n",
        "# ----------------------------\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "def get_val_loader(split=\"val\", batch=BATCH_EVAL, max_samples=None):\n",
        "    folder = os.path.join(DATA_ROOT, split)\n",
        "    if os.path.isdir(folder):\n",
        "        ds = datasets.ImageFolder(folder, transform=preprocess)\n",
        "        if max_samples:\n",
        "            ds = torch.utils.data.Subset(ds, list(range(min(max_samples, len(ds)))))\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=False, num_workers=2)\n",
        "    else:\n",
        "        # fallback: FakeData\n",
        "        from torchvision.datasets import FakeData\n",
        "        ds = FakeData(size=1024, image_size=(3, IMAGE_SIZE, IMAGE_SIZE), transform=preprocess)\n",
        "        if max_samples:\n",
        "            ds = torch.utils.data.Subset(ds, list(range(min(max_samples, len(ds)))))\n",
        "        return DataLoader(ds, batch_size=batch, shuffle=False, num_workers=2)\n",
        "\n",
        "def collect_calib_image_paths(root=DATA_ROOT, split=\"val\", max_files=CALIB_SAMPLES):\n",
        "    folder = os.path.join(root, split)\n",
        "    files = []\n",
        "    if not os.path.isdir(folder):\n",
        "        raise FileNotFoundError(f\"Calibration folder {folder} not found: update DATA_ROOT\")\n",
        "    for cls in os.listdir(folder):\n",
        "        cls_folder = os.path.join(folder, cls)\n",
        "        if not os.path.isdir(cls_folder):\n",
        "            continue\n",
        "        for f in os.listdir(cls_folder):\n",
        "            if f.lower().endswith((\".jpg\", \".jpeg\", \".png\")):\n",
        "                files.append(os.path.join(cls_folder, f))\n",
        "                if len(files) >= max_files:\n",
        "                    return files\n",
        "    return files\n",
        "\n",
        "# ----------------------------\n",
        "# EXPORT FP32 ONNX with dynamic axes (batch dynamic)\n",
        "# ----------------------------\n",
        "def export_pytorch_to_onnx(model, out_path=FP32_ONNX, opset=OPSET):\n",
        "    model = model.eval().cpu()\n",
        "    dummy = torch.randn(1,3,IMAGE_SIZE,IMAGE_SIZE, dtype=torch.float32)\n",
        "    torch.onnx.export(\n",
        "        model, dummy, out_path,\n",
        "        input_names=[\"input\"], output_names=[\"output\"],\n",
        "        dynamic_axes={\"input\": {0: \"batch\"}, \"output\": {0: \"batch\"}},\n",
        "        opset_version=opset, do_constant_folding=True\n",
        "    )\n",
        "    print(\"Exported FP32 ONNX:\", out_path)\n",
        "\n",
        "# ----------------------------\n",
        "# CalibrationDataReader for quantize_static (reads image files)\n",
        "# ----------------------------\n",
        "class ImageCalibrationReader(CalibrationDataReader):\n",
        "    def __init__(self, onnx_model_path, image_files, batch_size=CALIB_BATCH):\n",
        "        self.session = ort.InferenceSession(onnx_model_path, providers=[\"CPUExecutionProvider\"])\n",
        "        self.input_name = self.session.get_inputs()[0].name\n",
        "        self.files = image_files\n",
        "        self.batch_size = batch_size\n",
        "        self.idx = 0\n",
        "        self.transform = preprocess\n",
        "\n",
        "    def get_next(self):\n",
        "        if self.idx >= len(self.files):\n",
        "            return None\n",
        "        batch_files = self.files[self.idx:self.idx + self.batch_size]\n",
        "        arrs = []\n",
        "        for p in batch_files:\n",
        "            img = Image.open(p).convert(\"RGB\")\n",
        "            x = self.transform(img).unsqueeze(0).numpy().astype(np.float32)\n",
        "            arrs.append(x)\n",
        "        self.idx += self.batch_size\n",
        "        batch = np.concatenate(arrs, axis=0)\n",
        "        return {self.input_name: batch}\n",
        "\n",
        "# ----------------------------\n",
        "# PROFILE: execution order, per-module latency & activation memory\n",
        "# ----------------------------\n",
        "def is_leaf_module(m):\n",
        "    return len(list(m.children())) == 0\n",
        "\n",
        "def tensor_bytes(t):\n",
        "    return t.numel() * t.element_size()\n",
        "\n",
        "def profile_forward_order(model, sample_input, runs=8):\n",
        "    model = model.eval()\n",
        "    device = next(model.parameters()).device if any(True for _ in model.parameters()) else torch.device(\"cpu\")\n",
        "    x = sample_input.to(device)\n",
        "\n",
        "    start_times = {}\n",
        "    total_time = defaultdict(float)\n",
        "    total_mem = defaultdict(float)\n",
        "    calls = defaultdict(int)\n",
        "    exec_order = []\n",
        "    name_map = {}\n",
        "\n",
        "    def pre_hook(m, inp):\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        start_times[id(m)] = time.perf_counter()\n",
        "\n",
        "    def post_hook(m, inp, out):\n",
        "        if device.type == \"cuda\":\n",
        "            torch.cuda.synchronize()\n",
        "        elapsed = time.perf_counter() - start_times[id(m)]\n",
        "        total_time[m] += elapsed\n",
        "        calls[m] += 1\n",
        "        # activation bytes\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            total_mem[m] += tensor_bytes(out.detach().cpu())\n",
        "        elif isinstance(out, (list, tuple)):\n",
        "            s = 0\n",
        "            for o in out:\n",
        "                if isinstance(o, torch.Tensor):\n",
        "                    s += tensor_bytes(o.detach().cpu())\n",
        "            total_mem[m] += s\n",
        "        if m not in exec_order:\n",
        "            exec_order.append(m)\n",
        "\n",
        "    hooks = []\n",
        "    name_map = {mod: name for name, mod in model.named_modules()}\n",
        "    for name, mod in model.named_modules():\n",
        "        if is_leaf_module(mod):\n",
        "            hooks.append(mod.register_forward_pre_hook(pre_hook))\n",
        "            hooks.append(mod.register_forward_hook(post_hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # warmup\n",
        "        for _ in range(2):\n",
        "            model(x)\n",
        "        # runs\n",
        "        for _ in range(runs):\n",
        "            model(x)\n",
        "\n",
        "    for h in hooks: h.remove()\n",
        "\n",
        "    avg_time = {m: total_time[m] / calls[m] for m in total_time}\n",
        "    avg_mem = {m: total_mem[m] / calls[m] for m in total_mem}\n",
        "    return exec_order, avg_time, avg_mem, name_map\n",
        "\n",
        "# ----------------------------\n",
        "# SENSITIVITY: L2 output change when quantizing module weights (weights-only simulation)\n",
        "# ----------------------------\n",
        "def quantize_tensor_sym(x, num_bits=8):\n",
        "    if num_bits >= 32:\n",
        "        return x.clone()\n",
        "    qmax = 2 ** (num_bits - 1) - 1\n",
        "    t_abs = x.abs().max()\n",
        "    if t_abs == 0:\n",
        "        return x.clone()\n",
        "    scale = t_abs / qmax\n",
        "    q = torch.clamp((x / scale).round(), -qmax, qmax)\n",
        "    return q * scale\n",
        "\n",
        "def layer_sensitivity_l2(model, module, sample_input, num_bits=8):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device if any(True for _ in model.parameters()) else torch.device(\"cpu\")\n",
        "    x = sample_input.to(device)\n",
        "\n",
        "    orig = None\n",
        "    def hook_orig(m, inp, out):\n",
        "        nonlocal orig\n",
        "        orig = out.detach().cpu().clone()\n",
        "    h1 = module.register_forward_hook(hook_orig)\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "    h1.remove()\n",
        "    if orig is None:\n",
        "        return float(\"inf\")\n",
        "\n",
        "    # backup & quantize inplace\n",
        "    backup = {}\n",
        "    for name, p in module.named_parameters(recurse=False):\n",
        "        backup[name] = p.data.clone()\n",
        "        p.data = quantize_tensor_sym(p.data, num_bits=num_bits).to(p.device)\n",
        "\n",
        "    pert = None\n",
        "    def hook_pert(m, inp, out):\n",
        "        nonlocal pert\n",
        "        pert = out.detach().cpu().clone()\n",
        "    h2 = module.register_forward_hook(hook_pert)\n",
        "    with torch.no_grad():\n",
        "        model(x)\n",
        "    h2.remove()\n",
        "\n",
        "    # restore\n",
        "    for name, p in module.named_parameters(recurse=False):\n",
        "        p.data.copy_(backup[name])\n",
        "\n",
        "    if pert is None:\n",
        "        return float(\"inf\")\n",
        "    diff = (orig - pert).view(-1)\n",
        "    l2 = torch.norm(diff).item()\n",
        "    return l2 / (orig.numel() + 1e-12)\n",
        "\n",
        "# ----------------------------\n",
        "# SCORE computation & selection\n",
        "# ----------------------------\n",
        "def compute_scores(exec_order, avg_time, avg_mem, name_map, model, sample_input):\n",
        "    candidates = []\n",
        "    lat_list, mem_list, sens_list = [], [], []\n",
        "    infos = []\n",
        "\n",
        "    for m in exec_order:\n",
        "        # only modules with parameters\n",
        "        if any(True for _ in m.parameters(recurse=False)):\n",
        "            t = float(avg_time.get(m, 0.0))\n",
        "            mem = float(avg_mem.get(m, 0.0)) + sum(p.numel() * p.element_size() for p in m.parameters(recurse=False))\n",
        "            sens = float(layer_sensitivity_l2(model, m, sample_input))\n",
        "            lat_list.append(t); mem_list.append(mem); sens_list.append(sens)\n",
        "            infos.append((m, name_map.get(m, \"<noname>\"), t, mem, sens))\n",
        "\n",
        "    if len(infos) == 0:\n",
        "        return []\n",
        "\n",
        "    def norm(arr):\n",
        "        mn = min(arr); mx = max(arr)\n",
        "        if mx - mn < 1e-12:\n",
        "            return [0.0 for _ in arr]\n",
        "        return [(v - mn) / (mx - mn) for v in arr]\n",
        "\n",
        "    lat_n = norm(lat_list); mem_n = norm(mem_list); sens_n = norm(sens_list)\n",
        "\n",
        "    scored = []\n",
        "    for i, (m, nm, t, mem, sens) in enumerate(infos):\n",
        "        score = (lat_n[i] + mem_n[i]) / (sens_n[i] + 1e-9)\n",
        "        scored.append((score, m, nm, t, mem, sens))\n",
        "    scored.sort(reverse=True, key=lambda x: x[0])\n",
        "    return scored\n",
        "\n",
        "# ----------------------------\n",
        "# MAP selected PyTorch modules -> ONNX Conv/Gemm nodes (order heuristic)\n",
        "# ----------------------------\n",
        "def map_modules_to_onnx_nodes(scored_list, onnx_path, max_map=None):\n",
        "    model = onnx.load(onnx_path)\n",
        "    onnx_nodes = [n for n in model.graph.node if n.op_type in (\"Conv\", \"Gemm\", \"MatMul\")]\n",
        "    onnx_names = [n.name if n.name != \"\" else f\"{n.op_type}_{i}\" for i, n in enumerate(onnx_nodes)]\n",
        "    # Map first N scored modules to first N ONNX conv/gemm nodes\n",
        "    N = len(scored_list) if max_map is None else min(len(scored_list), max_map)\n",
        "    mapped = onnx_names[:N]\n",
        "    return mapped\n",
        "\n",
        "# ----------------------------\n",
        "# SELECTIVE QUANTIZATION (QDQ static)\n",
        "# ----------------------------\n",
        "def quantize_static_selective(fp32_onnx, out_qonnx, calib_files, nodes_to_quantize, per_channel=True):\n",
        "    reader = ImageCalibrationReader(fp32_onnx, calib_files, batch_size=CALIB_BATCH)\n",
        "    print(f\"Quantizing (QDQ) selected nodes count={len(nodes_to_quantize)} ...\")\n",
        "    try:\n",
        "        quantize_static(\n",
        "            model_input=fp32_onnx,\n",
        "            model_output=out_qonnx,\n",
        "            calibration_data_reader=reader,\n",
        "            quant_format=QuantFormat.QDQ,\n",
        "            activation_type=QuantType.QInt8,\n",
        "            weight_type=QuantType.QInt8,\n",
        "            per_channel=per_channel,\n",
        "            nodes_to_quantize=nodes_to_quantize\n",
        "        )\n",
        "        print(\"Saved selective QDQ model:\", out_qonnx)\n",
        "    except TypeError as e:\n",
        "        # nodes_to_quantize arg might not exist in some ORT versions\n",
        "        print(\"quantize_static selective failed (nodes_to_quantize not supported). Falling back to full QDQ.\")\n",
        "        quantize_static(\n",
        "            model_input=fp32_onnx,\n",
        "            model_output=out_qonnx,\n",
        "            calibration_data_reader=reader,\n",
        "            quant_format=QuantFormat.QDQ,\n",
        "            activation_type=QuantType.QInt8,\n",
        "            weight_type=QuantType.QInt8,\n",
        "            per_channel=per_channel\n",
        "        )\n",
        "        print(\"Saved full QDQ model as fallback:\", out_qonnx)\n",
        "\n",
        "# ----------------------------\n",
        "# EVALUATE ONNX (accuracy, latency, size)\n",
        "# ----------------------------\n",
        "def evaluate_onnx_model(onnx_path, loader, provider=\"CPUExecutionProvider\", max_images=EVAL_SAMPLES):\n",
        "    sess = ort.InferenceSession(onnx_path, providers=[provider])\n",
        "    input_name = sess.get_inputs()[0].name\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    times = []\n",
        "    for images, labels in loader:\n",
        "        imgs = images.numpy().astype(np.float32)\n",
        "        t0 = time.time()\n",
        "        outputs = sess.run(None, {input_name: imgs})[0]\n",
        "        t1 = time.time()\n",
        "        preds = np.argmax(outputs, axis=1)\n",
        "        correct += (preds == labels.numpy()).sum()\n",
        "        total += labels.size(0)\n",
        "        times.append(t1 - t0)\n",
        "        if total >= max_images:\n",
        "            break\n",
        "    acc = correct / total if total>0 else 0.0\n",
        "    latency_ms = (sum(times) / len(times)) * 1000.0 if len(times)>0 else 0.0\n",
        "    size_mb = os.path.getsize(onnx_path) / (1024*1024)\n",
        "    return acc, latency_ms, size_mb\n",
        "\n",
        "# ----------------------------\n",
        "# MAIN PIPELINE\n",
        "# ----------------------------\n",
        "def main():\n",
        "    print(\"Loading PyTorch model (ResNet18 pretrained)...\")\n",
        "    pt_model = models.resnet18(weights=\"IMAGENET1K_V1\").to(\"cpu\").eval()\n",
        "\n",
        "    # sample for profiling: 8 images\n",
        "    val_small_loader = get_val_loader(\"val\", batch=8, max_samples=8)\n",
        "    sample_images, _ = next(iter(val_small_loader))\n",
        "    sample_images = sample_images[:8]\n",
        "\n",
        "    print(\"Profiling forward order / latency / activation memory...\")\n",
        "    exec_order, avg_time_map, avg_mem_map, name_map = profile_forward_order(pt_model, sample_images, runs=8)\n",
        "    print(f\"Leaf modules executed: {len(exec_order)}\")\n",
        "\n",
        "    print(\"Computing sensitivity & scores (this may take time)...\")\n",
        "    scored = compute_scores(exec_order, avg_time_map, avg_mem_map, name_map, pt_model, sample_images)\n",
        "    print(\"Top 10 scored modules:\")\n",
        "    for s, m, nm, t, mem, sens in scored[:10]:\n",
        "        print(f\"{nm:40s} | score={s:.3e} | lat={t:.6f}s | mem={mem/1024:.1f}KB | sens={sens:.3e}\")\n",
        "\n",
        "    print(\"Exporting FP32 model to ONNX with dynamic batch...\")\n",
        "    export_pytorch_to_onnx(pt_model, FP32_ONNX)\n",
        "\n",
        "    print(\"Collecting calibration images...\")\n",
        "    calib_files = collect_calib_image_paths(root=DATA_ROOT, split=\"val\", max_files=CALIB_SAMPLES)\n",
        "    print(f\"Calibration images: {len(calib_files)}\")\n",
        "\n",
        "    # optional: do a full QLinear quant (comment if you don't want)\n",
        "    # print(\"Running full QLinear static quant (optional)...\")\n",
        "    # run_full_qlinear = False\n",
        "    # if run_full_qlinear:\n",
        "    #     # You can use quantize_static (QOperator) to produce QONNX_FULL\n",
        "    #     pass\n",
        "\n",
        "    # prepare evaluation loader (val)\n",
        "    eval_loader = get_val_loader(\"val\", batch=BATCH_EVAL, max_samples=EVAL_SAMPLES)\n",
        "\n",
        "    print(\"Evaluate FP32 ONNX baseline...\")\n",
        "    fp32_acc, fp32_lat, fp32_size = evaluate_onnx_model(FP32_ONNX, eval_loader)\n",
        "    print(f\"FP32: Acc={fp32_acc:.4f}, Latency={fp32_lat:.2f} ms, Size={fp32_size:.2f} MB\")\n",
        "\n",
        "    # sweep K (number of top-scored modules to quantize)\n",
        "    max_map = min(len(scored),  len([n for n in onnx.load(FP32_ONNX).graph.node if n.op_type in (\"Conv\",\"Gemm\",\"MatMul\")]))\n",
        "    ks = list(sorted(set([0, 1, 2, 5, 10, 20, max_map])))  # tune as needed\n",
        "    ks = [k for k in ks if k <= max_map]\n",
        "\n",
        "    results = []\n",
        "    for k in ks:\n",
        "        print(\"==== Sweep K =\", k, \"====\")\n",
        "        topk = scored[:k]\n",
        "        # map to ONNX node names by order heuristic\n",
        "        onnx_nodes_to_quantize = map_modules_to_onnx_nodes(topk, FP32_ONNX, max_map=k)\n",
        "        print(f\"Mapped {len(onnx_nodes_to_quantize)} ONNX nodes for quantization.\")\n",
        "\n",
        "        out_q = QONNX_SEL_TEMPLATE.format(k)\n",
        "        quantize_static_selective(FP32_ONNX, out_q, calib_files, onnx_nodes_to_quantize, per_channel=True)\n",
        "\n",
        "        acc, lat, size = evaluate_onnx_model(out_q, eval_loader)\n",
        "        print(f\"K={k} -> Acc={acc:.4f}, Lat={lat:.2f} ms, Size={size:.2f} MB\")\n",
        "        results.append({\"K\": k, \"accuracy\": acc, \"latency_ms\": lat, \"size_mb\": size})\n",
        "\n",
        "    # include FP32 baseline at K=0 (already included if 0 in ks)\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(\"selective_qdq_sweep_results.csv\", index=False)\n",
        "    print(\"Saved sweep results to selective_qdq_sweep_results.csv\")\n",
        "    print(df)\n",
        "\n",
        "    # PLOT\n",
        "    plt.figure(figsize=(6,4)); plt.plot(df[\"K\"], df[\"accuracy\"], marker='o'); plt.xlabel(\"K (quantized nodes)\"); plt.ylabel(\"Top-1 Accuracy\"); plt.grid(True); plt.title(\"Accuracy vs K\"); plt.show()\n",
        "    plt.figure(figsize=(6,4)); plt.plot(df[\"K\"], df[\"latency_ms\"], marker='o'); plt.xlabel(\"K (quantized nodes)\"); plt.ylabel(\"Latency (ms)\"); plt.grid(True); plt.title(\"Latency vs K\"); plt.show()\n",
        "    plt.figure(figsize=(6,4)); plt.plot(df[\"K\"], df[\"size_mb\"], marker='o'); plt.xlabel(\"K (quantized nodes)\"); plt.ylabel(\"On-disk Size (MB)\"); plt.grid(True); plt.title(\"Model Size vs K\"); plt.show()\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 974
        },
        "id": "9E-sp-oQqBhn",
        "outputId": "fa84a124-31fd-4ad9-f86a-428f25ef9d84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Profiling model...\n",
            "Selecting layers...\n",
            "Selected: ['bn1', 'layer1.1.bn1', 'layer1.0.bn1', 'layer1.0.bn2', 'layer1.1.bn2', 'layer2.0.bn1', 'layer2.0.bn2', 'layer2.1.bn1', 'layer1.1.conv2', 'layer2.0.downsample.1', 'layer2.1.bn2', 'conv1']  total layers: 41\n",
            "Exporting ONNX...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3764657906.py:161: UserWarning: # 'dynamic_axes' is not recommended when dynamo=True, and may lead to 'torch._dynamo.exc.UserError: Constraints violated.' Supply the 'dynamic_shapes' argument instead if export is unsuccessful.\n",
            "  torch.onnx.export(model, dummy, \"model.onnx\",\n",
            "W1128 19:53:13.354000 193 torch/onnx/_internal/exporter/_compat.py:114] Setting ONNX exporter to use operator set version 18 because the requested opset_version 13 is a lower version than we have implementations for. Automatic version conversion will be performed, which may not be successful at converting to the requested version. If version conversion is unsuccessful, the opset version of the exported model will be kept at 18. Please consider setting opset_version >=18 to leverage latest ONNX features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`...\n",
            "[torch.onnx] Obtain model graph for `ResNet([...]` with `torch.export.export(..., strict=False)`... ✅\n",
            "[torch.onnx] Run decomposition...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:onnxscript.version_converter:The model version conversion is not supported by the onnxscript version converter and fallback is enabled. The model will be converted using the onnx C API (target version: 13).\n",
            "WARNING:onnxscript.version_converter:Failed to convert the model to the target version 13 using the ONNX C API. The model was not modified\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 127, in call\n",
            "    converted_proto = _c_api_utils.call_onnx_api(\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/_c_api_utils.py\", line 65, in call_onnx_api\n",
            "    result = func(proto)\n",
            "             ^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnxscript/version_converter/__init__.py\", line 122, in _partial_convert_version\n",
            "    return onnx.version_converter.convert_version(\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/onnx/version_converter.py\", line 39, in convert_version\n",
            "    converted_model_str = C.convert_version(model_str, target_version)\n",
            "                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "RuntimeError: /github/workspace/onnx/version_converter/adapters/axes_input_to_attribute.h:65: adapt: Assertion `node->hasAttribute(kaxes)` failed: No initializer or constant input to node found\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[torch.onnx] Run decomposition... ✅\n",
            "[torch.onnx] Translate the graph into ONNX...\n",
            "[torch.onnx] Translate the graph into ONNX... ✅\n",
            "Applied 40 of general pattern rewrite rules.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Exported model.onnx\n",
            "Quantizing selectively (INT8 QLinear)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Please consider pre-processing before quantization. See https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n",
            "WARNING:root:Please consider to run pre-processing before quantization. Refer to example: https://github.com/microsoft/onnxruntime-inference-examples/blob/main/quantization/image_classification/cpu/ReadMe.md \n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "No data is collected.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3764657906.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    264\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"==============================\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-3764657906.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    249\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Quantizing selectively (INT8 QLinear)...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m     \u001b[0mselective_quantize_static\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"model_int8.onnx\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    252\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Evaluating FP32...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3764657906.py\u001b[0m in \u001b[0;36mselective_quantize_static\u001b[0;34m(selected_nodes, output)\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0mnodes_to_quantize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     )\n\u001b[0;32m--> 197\u001b[0;31m     quantize_static(\n\u001b[0m\u001b[1;32m    198\u001b[0m     \u001b[0mmodel_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0mmodel_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"model_int8.onnx\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/quantize.py\u001b[0m in \u001b[0;36mquantize_static\u001b[0;34m(model_input, model_output, calibration_data_reader, quant_format, op_types_to_quantize, per_channel, reduce_range, activation_type, weight_type, nodes_to_quantize, nodes_to_exclude, use_external_data_format, calibrate_method, calibration_providers, extra_options)\u001b[0m\n\u001b[1;32m    740\u001b[0m                 \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_data_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    741\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 742\u001b[0;31m             \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcalibration_data_reader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    743\u001b[0m         \u001b[0mtensors_range\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcalibrator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    744\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors_range\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTensorsData\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/onnxruntime/quantization/calibrate.py\u001b[0m in \u001b[0;36mcollect_data\u001b[0;34m(self, data_reader)\u001b[0m\n\u001b[1;32m    426\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_outputs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalibrate_tensors_range\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 428\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No data is collected.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    430\u001b[0m         \u001b[0mt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: No data is collected."
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torchvision import models, datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "import time\n",
        "from collections import defaultdict\n",
        "\n",
        "import onnx\n",
        "import onnxruntime\n",
        "from onnxruntime.quantization import quantize_static, CalibrationDataReader, QuantType\n",
        "\n",
        "################################################################################\n",
        "# 0. CONFIGURATION\n",
        "################################################################################\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "IMAGE_SIZE = 224\n",
        "BATCH_SIZE = 1\n",
        "\n",
        "DATA_ROOT = \"/kaggle/input/imagenetmini-1000/imagenet-mini/train\"\n",
        "#DATA_ROOT = \"/root/.cache/kagglehub/datasets/ifigotin/imagenetmini-1000/versions/1/imagenet-mini/train\"\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((IMAGE_SIZE, IMAGE_SIZE)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
        "])\n",
        "\n",
        "dataset = datasets.ImageFolder(DATA_ROOT, transform)\n",
        "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "model = models.resnet18(weights=\"IMAGENET1K_V1\").eval().to(DEVICE)\n",
        "\n",
        "################################################################################\n",
        "# 1. BASIC UTILITY FUNCTIONS\n",
        "################################################################################\n",
        "\n",
        "def is_leaf(m):\n",
        "    return (len(list(m.children())) == 0)\n",
        "\n",
        "def tensor_bytes(t):\n",
        "    return t.numel() * t.element_size()\n",
        "\n",
        "def param_bytes(m):\n",
        "    return sum(p.numel() * p.element_size() for p in m.parameters(recurse=False))\n",
        "\n",
        "################################################################################\n",
        "# 2. PROFILE LAYER LATENCY + MEMORY\n",
        "################################################################################\n",
        "def profile_layers(model, x):\n",
        "    model.eval()\n",
        "    x = x.to(DEVICE)\n",
        "\n",
        "    names = {m: n for n, m in model.named_modules()}\n",
        "    lat, mem, calls = defaultdict(float), defaultdict(float), defaultdict(int)\n",
        "\n",
        "    start_times = {}\n",
        "\n",
        "    def pre_hook(m, inp):\n",
        "        torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
        "        start_times[m] = time.perf_counter()\n",
        "\n",
        "    def fwd_hook(m, inp, out):\n",
        "        torch.cuda.synchronize() if DEVICE == \"cuda\" else None\n",
        "        dt = time.perf_counter() - start_times[m]\n",
        "        lat[m] += dt\n",
        "        calls[m] += 1\n",
        "        if isinstance(out, torch.Tensor):\n",
        "            mem[m] += tensor_bytes(out.cpu())\n",
        "\n",
        "    hooks = []\n",
        "    for name, m in model.named_modules():\n",
        "        if is_leaf(m):\n",
        "            hooks.append(m.register_forward_pre_hook(pre_hook))\n",
        "            hooks.append(m.register_forward_hook(fwd_hook))\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(5):\n",
        "            model(x)\n",
        "\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "    for m in lat:\n",
        "        lat[m] /= calls[m]\n",
        "        mem[m] /= calls[m]\n",
        "\n",
        "    return lat, mem, names\n",
        "\n",
        "################################################################################\n",
        "# 3. SENSITIVITY (L2 output difference from quant-dequant weights)\n",
        "################################################################################\n",
        "def quant_dequant(w, num_bits=8):\n",
        "    qmax = 2**(num_bits-1) - 1\n",
        "    scale = w.abs().max() / qmax\n",
        "    return (w/scale).round().clamp(-qmax, qmax) * scale\n",
        "\n",
        "def layer_sensitivity(model, module, sample):\n",
        "    model.eval()\n",
        "    sample = sample.to(DEVICE)\n",
        "\n",
        "    orig_out = None\n",
        "    def h1(m, i, o):\n",
        "        nonlocal orig_out\n",
        "        orig_out = o.detach().cpu()\n",
        "\n",
        "    hook1 = module.register_forward_hook(h1)\n",
        "    with torch.no_grad(): model(sample)\n",
        "    hook1.remove()\n",
        "\n",
        "    if orig_out is None:\n",
        "        return float(\"inf\")\n",
        "\n",
        "    backup = {n: p.data.clone() for n, p in module.named_parameters(recurse=False)}\n",
        "\n",
        "    for n, p in module.named_parameters(recurse=False):\n",
        "        p.data = quant_dequant(p.data)\n",
        "\n",
        "    pert_out = None\n",
        "    def h2(m, i, o):\n",
        "        nonlocal pert_out\n",
        "        pert_out = o.detach().cpu()\n",
        "\n",
        "    hook2 = module.register_forward_hook(h2)\n",
        "    with torch.no_grad(): model(sample)\n",
        "    hook2.remove()\n",
        "\n",
        "    # restore\n",
        "    for n, p in module.named_parameters(recurse=False):\n",
        "        p.data.copy_(backup[n])\n",
        "\n",
        "    if pert_out is None:\n",
        "        return float(\"inf\")\n",
        "\n",
        "    diff = (orig_out - pert_out).view(-1)\n",
        "    return torch.norm(diff).item() / orig_out.numel()\n",
        "\n",
        "################################################################################\n",
        "# 4. SELECT BEST LAYERS FOR QUANTIZATION\n",
        "################################################################################\n",
        "def select_layers(model, lat, mem, names, sample, K=10):\n",
        "    scores = []\n",
        "\n",
        "    for m in lat:\n",
        "        if any(True for _ in m.parameters(recurse=False)):\n",
        "            sens = layer_sensitivity(model, m, sample)\n",
        "            score = (lat[m] + mem[m]) / (sens + 1e-9)\n",
        "            scores.append((score, names[m]))\n",
        "\n",
        "    scores.sort(reverse=True)\n",
        "    selected = [name for _, name in scores[:K]]\n",
        "    return selected, scores\n",
        "\n",
        "################################################################################\n",
        "# 5. EXPORT ONNX\n",
        "################################################################################\n",
        "def export_onnx(model):\n",
        "    dummy = torch.randn(1,3,224,224).to(DEVICE)\n",
        "    torch.onnx.export(model, dummy, \"model.onnx\",\n",
        "                      input_names=[\"input\"],\n",
        "                      output_names=[\"output\"],\n",
        "                      opset_version=13,\n",
        "                      dynamic_axes={\"input\": {0:\"batch\"},\n",
        "                                    \"output\": {0:\"batch\"}})\n",
        "    print(\"Exported model.onnx\")\n",
        "\n",
        "################################################################################\n",
        "# 6. DUMMY CALIBRATION FOR STATIC QUANTIZATION\n",
        "################################################################################\n",
        "class DummyDataReader(CalibrationDataReader):\n",
        "    def __init__(self):\n",
        "        self.batch = {\"input\": np.random.randn(1,3,224,224).astype(np.float32)}\n",
        "        self.done = False\n",
        "    def get_next(self):\n",
        "        if self.done:\n",
        "            return None\n",
        "        self.done = True\n",
        "        return self.batch\n",
        "\n",
        "################################################################################\n",
        "# 7. RUN STATIC QUANTIZATION (WITH SELECTIVE LAYER LIST)\n",
        "################################################################################\n",
        "def selective_quantize_static(selected_nodes, output=\"model_int8.onnx\"):\n",
        "    dr = DummyDataReader()  # no need to load val dataset\n",
        "\n",
        "    quantize_static(\n",
        "        model_input=\"model.onnx\",\n",
        "        model_output=output,\n",
        "        calibration_data_reader=dr,\n",
        "        quant_format=\"QLinearOps\",\n",
        "        activation_type=QuantType.QUInt8,\n",
        "        weight_type=QuantType.QInt8,\n",
        "        nodes_to_quantize=selected_nodes\n",
        "    )\n",
        "    quantize_static(\n",
        "    model_input=\"model.onnx\",\n",
        "    model_output=\"model_int8.onnx\",\n",
        "    calibration_data_reader=dr,\n",
        "    quant_format=QuantFormat.QOperator,        # <--- IMPORTANT\n",
        "    activation_type=QuantType.QUInt8,\n",
        "    weight_type=QuantType.QInt8,\n",
        ")\n",
        "\n",
        "    print(\"Saved quantized model as:\", output)\n",
        "\n",
        "################################################################################\n",
        "# 8. ONNX INFERENCE\n",
        "################################################################################\n",
        "def eval_onnx(path, loader):\n",
        "    sess = onnxruntime.InferenceSession(path, providers=[\"CPUExecutionProvider\"])\n",
        "    name = sess.get_inputs()[0].name\n",
        "\n",
        "    correct, total, lat = 0, 0, 0\n",
        "\n",
        "    for x, y in loader:\n",
        "        x = x.numpy()\n",
        "\n",
        "        t0 = time.perf_counter()\n",
        "        out = sess.run(None, {name: x})[0]\n",
        "        lat += time.perf_counter() - t0\n",
        "\n",
        "        pred = out.argmax(1)\n",
        "        correct += (pred == y.numpy()).sum()\n",
        "        total += len(y)\n",
        "\n",
        "        if total > 200:  # speed up\n",
        "            break\n",
        "\n",
        "    return correct/total, lat/total\n",
        "\n",
        "################################################################################\n",
        "# 9. MAIN\n",
        "################################################################################\n",
        "def main():\n",
        "    sample, _ = next(iter(dataloader))\n",
        "    sample = sample[:8]\n",
        "\n",
        "    print(\"Profiling model...\")\n",
        "    lat, mem, names = profile_layers(model, sample)\n",
        "\n",
        "    print(\"Selecting layers...\")\n",
        "    selected, scores = select_layers(model, lat, mem, names, sample, K=12)\n",
        "    print(\"Selected:\", selected,\" total layers:\",len(scores))\n",
        "\n",
        "    print(\"Exporting ONNX...\")\n",
        "    export_onnx(model)\n",
        "\n",
        "    print(\"Quantizing selectively (INT8 QLinear)...\")\n",
        "    selective_quantize_static(selected, \"model_int8.onnx\")\n",
        "\n",
        "    print(\"Evaluating FP32...\")\n",
        "    acc_fp32, lat_fp32 = eval_onnx(\"model.onnx\", dataloader)\n",
        "\n",
        "    print(\"Evaluating INT8...\")\n",
        "    acc_int8, lat_int8 = eval_onnx(\"model_int8.onnx\", dataloader)\n",
        "\n",
        "    print(\"\\n==============================\")\n",
        "    print(\" FP32  : Acc =\", acc_fp32, \", Lat =\", lat_fp32*1000, \"ms/img\")\n",
        "    print(\" INT8  : Acc =\", acc_int8, \", Lat =\", lat_int8*1000, \"ms/img\")\n",
        "    print(\"Size FP32 :\", os.path.getsize(\"model.onnx\")/1e6, \"MB\")\n",
        "    print(\"Size INT8 :\", os.path.getsize(\"model_int8.onnx\")/1e6, \"MB\")\n",
        "    print(\"==============================\")\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y9gQWA-Muv_f"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}